{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HybridModel_ProtoType.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW0I7m0fiUQL"
      },
      "source": [
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
        "!pip install fastai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWS-jkO1iVdV"
      },
      "source": [
        "!apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \\\n",
        "flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig libpulse-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_1wWh4iiVqa"
      },
      "source": [
        "!pip install text-preprocessing\n",
        "!pip install textract\n",
        "!pip install azure-storage-blob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Zr8CYqiqRd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords', quiet = True)\n",
        "nltk.download('punkt', quiet = True)\n",
        "nltk.download('words',quiet = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBjDMgJwiqyO"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im1JNBIai2tZ"
      },
      "source": [
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.text import * \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from text_preprocessing import preprocess_text\n",
        "import textract\n",
        "from functools import partial\n",
        "import re\n",
        "import io\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5j4GuRYjTUx"
      },
      "source": [
        "**MODEL 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpuF7NQQiOSs"
      },
      "source": [
        "def unique_list(l):\n",
        "    ulist = []\n",
        "    [ulist.append(x) for x in l if x not in ulist]\n",
        "    return ulist\n",
        "\n",
        "def process(rootdir, train = True):\n",
        "  #File path.\n",
        "  paths = []\n",
        "  #File name. \n",
        "  fname = []\n",
        "  #Textracted content from file.\n",
        "  descr = []\n",
        "  #Labels of file.\n",
        "  label = []\n",
        "  #Length of each document.\n",
        "  length = []\n",
        "  # Store the list of stop words in english language.\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # Store the list of words common in english language.  \n",
        "  words = set(nltk.corpus.words.words())\n",
        "\n",
        "  # Walking through folders & subfolders in the root directory.\n",
        "  for subdir, dirs, files in os.walk(rootdir):\n",
        "    extract_label = subdir.split('/')\n",
        "    for file in files:\n",
        "        # Append the path of the file to path variable.\n",
        "        paths.append(os.path.join(subdir, file))\n",
        "        # Append the filename to the filename variable.\n",
        "        fname.append(str(file))\n",
        "        # Append the label to label variable.\n",
        "        label.append(int(extract_label[-1])) #[Optional May Change Depending on your folder structure].\n",
        "\n",
        "        # Extract the text from the files and decode the byte string to text.\n",
        "        text = textract.process(os.path.join(subdir, file)).decode(\"utf-8\") \n",
        "        \n",
        "        # Preprocess the text with a custom library. (Includes stemming, lemmatization, removal of special characters).\n",
        "        t = preprocess_text(text)\n",
        "  \n",
        "        # Tokenize the text.\n",
        "        word_tokens = word_tokenize(t)  \n",
        "  \n",
        "        # Apply a filter to the text which removes stop words.\n",
        "        filtered_sentence = [w for w in word_tokens if w not in stop_words]  \n",
        "        \n",
        "        # Remove charcaters which are not properly processed in the text.\n",
        "        filtered_sentence = [w for w in filtered_sentence if len(w) > 3]\n",
        "\n",
        "        # Remove any numeric characters that got included in text\n",
        "        filtered_sentence = [''.join(x for x in i if x.isalpha()) for i in filtered_sentence]\n",
        "\n",
        "        # Join the tokens with space(' ') as delimiter.\n",
        "        filtered_sentence = \" \".join(filtered_sentence)\n",
        "\n",
        "        # Remove extra spaces in the text.\n",
        "        res = re.sub(' +', ' ', filtered_sentence) \n",
        "\n",
        "        # Join the text.\n",
        "        a=' '.join(unique_list(res.split()))\n",
        "\n",
        "        # Remove the words that are not present in english language.\n",
        "        a = \" \".join(w for w in nltk.wordpunct_tokenize(a) \\\n",
        "              if w.lower() in words or not w.isalpha())\n",
        "        \n",
        "        # Append the text to description variable.\n",
        "        descr.append(a)\n",
        "\n",
        "        # Append the length of text to length variable.\n",
        "        length.append(len(a))\n",
        "    \n",
        "  if train:\n",
        "    # Converting the target variable to a numpy array.\n",
        "    label = np.array(label)\n",
        "  \n",
        "  return {\"FileName\" : fname, \"FilePath\" : paths, \"Text\" : descr ,\"Label\" : label, \"Length\" : length}\n",
        "\n",
        "\n",
        "def preprocess(Data_Frame):\n",
        "    #Dropping NaN values.\n",
        "    Data_Frame['Text'].isnull().sum()\n",
        "    Data_Frame.dropna(inplace = True)\n",
        "\n",
        "    # Remove column names 'FileName' & 'FilePath from Dataframe for training. \n",
        "    Data_Frame.drop(['Length', 'Label'], axis = 1, inplace = True)\n",
        "\n",
        "    return Data_Frame\n",
        "\n",
        "def model_1(rootdir, search, labels, rlabels):\n",
        "\n",
        "    learn      = load_learner('/home/yaswant/code/console/python/m1/') # Fixed path for model.\n",
        "\n",
        "    Data        =  process(rootdir)\n",
        "    Data_Frame  =  pd.DataFrame(Data, columns = ['FileName', 'FilePath', 'Text' ,'Label', 'Length'])\n",
        "    \n",
        "    Data_Frame = preprocess(Data_Frame)\n",
        "\n",
        "    target = []\n",
        "    for i in range(len(Data_Frame)) : \n",
        "        target.append(learn.predict(str(Data_Frame.loc[i, \"Text\"])))\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(target)):\n",
        "        target1 = target[i][0]\n",
        "        res = int(\"\".join(re.findall(r'\\d+', str(target1))))\n",
        "        result.append(labels[res])\n",
        "\n",
        "    Data_Frame[\"target\"] = result\n",
        "\n",
        "    if search == \"*\":\n",
        "        Data_Frame.to_csv('result.csv', encoding='utf-8', index = False)\n",
        "    else:\n",
        "        Data_Frame = Data_Frame.loc[Data_Frame['target'] == rlabels[search]]\n",
        "        #reseting index for test_data\n",
        "        Data_Frame.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return Data_Frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tG8bySgjXQE"
      },
      "source": [
        "**MODEL 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYde1c1hjL0n"
      },
      "source": [
        "def model_2(rootdir, search, labels, rlabels):\n",
        "\n",
        "    learn = load_learner('/home/yaswant/code/console/python/model2/') # Fixed path for model.\n",
        "\n",
        "    Data        =  process(rootdir)\n",
        "    Data_Frame  =  pd.DataFrame(Data, columns = ['FileName', 'FilePath', 'Text' ,'Label', 'Length'])\n",
        "    \n",
        "    Data_Frame = preprocess(Data_Frame)\n",
        "\n",
        "    target = []\n",
        "    for i in range(len(Data_Frame)) : \n",
        "        target.append(learn.predict(str(Data_Frame.loc[i, \"Text\"])))\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(target)):\n",
        "        target1 = target[i][0]\n",
        "        res = int(\"\".join(re.findall(r'\\d+', str(target1))))\n",
        "        result.append(labels[res])\n",
        "\n",
        "    Data_Frame[\"target\"] = result\n",
        "\n",
        "    if search == \"*\":\n",
        "        Data_Frame.to_csv('result.csv', encoding='utf-8', index = False)\n",
        "    else:\n",
        "        Data_Frame = Data_Frame.loc[Data_Frame['target'] == rlabels[search]]\n",
        "        #reseting index for test_data\n",
        "        Data_Frame.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return Data_Frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbFsBp9vji56"
      },
      "source": [
        "**ACTIVE LEARNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8WRCce-jgll"
      },
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import urllib.parse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPNMLHpujwZ1"
      },
      "source": [
        "def act_learn(rootdir, search, cont_name, folder):\n",
        "    paths = []\n",
        "    fname = []\n",
        "    search_term = urllib.parse.quote(str(search)) \n",
        "    print(\"File is being downloaded it may take upto 5 minutes\")\n",
        "    for subdir, dirs, files in os.walk(rootdir):\n",
        "        for file in files:\n",
        "            paths.append(os.path.join(subdir, file))\n",
        "            fname.append(str(file))\n",
        "    no_of_docs = int(len(fname))\n",
        "    \n",
        "    data1 = {\"FileName\" : fname, \"FilePath\" : paths, \"Label\" : [None] * len(paths)}\n",
        "    df1 = pd.DataFrame(data1, columns = ['FileName', 'FilePath', 'Label'])\n",
        "    \n",
        "    # Retrieve the connection string for use with the application. \n",
        "    connect_str = \"DefaultEndpointsProtocol=https;AccountName=eystrg;AccountKey=LY8gwsb1IAO2cY79EhfaA17BJHFSEzza0QE58L9nweCfiinr2ci+h9ZmCFjv92rRoHJkdQ/kl+7Aw6ti6BtdAQ==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "    # Create the BlobServiceClient object which will be used to create a container client\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "    for ind in df1.index:\n",
        "        #Uploading file to azure Blob Storage\n",
        "        upload_file_path = df1['FilePath'][ind]\n",
        "        local_file_name = str(folder) + \"/\" + df1['FileName'][ind]\n",
        "        # Specify the container (Dynamic Drop Down)\n",
        "        container_name = cont_name\n",
        "\n",
        "        # Create a blob client using the local file name as the name for the blob\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=local_file_name)\n",
        "\n",
        "        # Upload the created file\n",
        "        with open(upload_file_path, \"rb\") as data:\n",
        "            blob_client.upload_blob(data)\n",
        "\n",
        "    # Define the names for the data source, skillset, index and indexer\n",
        "    datasource_name = \"cogsrch-py-datasource\" + str(folder)\n",
        "    skillset_name = \"cogsrch-py-skillset\" + str(folder)\n",
        "    index_name = \"cogsrch-py-index\" + str(folder)\n",
        "    indexer_name = \"cogsrch-py-indexer\" + str(folder)\n",
        "\n",
        "    # Setup the endpoint\n",
        "    endpoint = 'https://eandysearch.search.windows.net'\n",
        "    headers = {'Content-Type': 'application/json',\n",
        "    'api-key': 'A15802993B784F745D50071F67BC731E'}\n",
        "    params = {'api-version': '2020-06-30'}\n",
        "    print(\"74\")\n",
        "    # Create a data source\n",
        "    datasourceConnectionString = connect_str\n",
        "    datasource_payload = {\n",
        "    \"name\": datasource_name,\n",
        "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
        "    \"type\": \"azureblob\",\n",
        "    \"credentials\": {\n",
        "        \"connectionString\": datasourceConnectionString\n",
        "    },\n",
        "    \"container\": {\n",
        "        \"name\": cont_name,\n",
        "        \"query\" : str(folder) + \"/\"\n",
        "    }}\n",
        "    r = requests.put(endpoint + \"/datasources/\" + datasource_name,\n",
        "                data=json.dumps(datasource_payload), headers=headers, params=params)\n",
        "    print(\"90\")\n",
        "    # Create a skillset\n",
        "    skillset_payload = {\n",
        "    \"name\": skillset_name,\n",
        "    \"description\":\n",
        "    \"Extract entities, detect language and extract key-phrases\",\n",
        "    \"skills\":\n",
        "    [\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.EntityRecognitionSkill\",\n",
        "            \"categories\": [\"Organization\"],\n",
        "            \"defaultLanguageCode\": \"en\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\", \n",
        "                    \"source\": \"/document/content\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"organizations\", \n",
        "                    \"targetName\": \"organizations\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\", \n",
        "                    \"source\": \"/document/content\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"languageCode\",\n",
        "                    \"targetName\": \"languageCode\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
        "            \"textSplitMode\": \"pages\",\n",
        "            \"maximumPageLength\": 4000,\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\",\n",
        "                    \"source\": \"/document/content\"\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"languageCode\",\n",
        "                    \"source\": \"/document/languageCode\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"textItems\",\n",
        "                    \"targetName\": \"pages\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\",\n",
        "            \"context\": \"/document/pages/*\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\", \n",
        "                    \"source\": \"/document/pages/*\"\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"languageCode\", \n",
        "                    \"source\": \"/document/languageCode\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"keyPhrases\",\n",
        "                    \"targetName\": \"keyPhrases\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]}\n",
        "\n",
        "    r = requests.put(endpoint + \"/skillsets/\" + skillset_name,\n",
        "                data=json.dumps(skillset_payload), headers=headers, params=params)\n",
        "    print(\"175\")\n",
        "    # Create an index\n",
        "    index_payload = {\n",
        "    \"name\": index_name,\n",
        "    \"fields\": [\n",
        "        {\n",
        "            \"name\": \"id\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"key\": \"true\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\",\n",
        "            \"sortable\": \"true\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"metadata_storage_name\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"facetable\": \"false\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"key\": \"false\",\n",
        "            \"retrievable\": \"true\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"analyzer\": \"standard.lucene\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"content\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"languageCode\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"keyPhrases\",\n",
        "            \"type\": \"Collection(Edm.String)\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"organizations\",\n",
        "            \"type\": \"Collection(Edm.String)\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        }\n",
        "    ]}\n",
        "\n",
        "    r = requests.put(endpoint + \"/indexes/\" + index_name,\n",
        "                data=json.dumps(index_payload), headers=headers, params=params)\n",
        "\n",
        "    print(\"235\")\n",
        "\n",
        "    # Create an indexer\n",
        "    indexer_payload = {\n",
        "    \"name\": indexer_name,\n",
        "    \"dataSourceName\": datasource_name,\n",
        "    \"targetIndexName\": index_name,\n",
        "    \"skillsetName\": skillset_name,\n",
        "    \"fieldMappings\": [\n",
        "        {\n",
        "            \"sourceFieldName\": \"metadata_storage_path\",\n",
        "            \"targetFieldName\": \"id\",\n",
        "            \"mappingFunction\":\n",
        "            {\"name\": \"base64Encode\"}\n",
        "        },\n",
        "        {\n",
        "            \"sourceFieldName\": \"content\",\n",
        "            \"targetFieldName\": \"content\"\n",
        "        }\n",
        "    ],\n",
        "    \"outputFieldMappings\":\n",
        "    [\n",
        "        {\n",
        "            \"sourceFieldName\": \"/document/organizations\",\n",
        "            \"targetFieldName\": \"organizations\"\n",
        "        },\n",
        "        {\n",
        "            \"sourceFieldName\": \"/document/pages/*/keyPhrases/*\",\n",
        "            \"targetFieldName\": \"keyPhrases\"\n",
        "        },\n",
        "        {\n",
        "            \"sourceFieldName\": \"/document/languageCode\",\n",
        "            \"targetFieldName\": \"languageCode\"\n",
        "        }\n",
        "    ],\n",
        "    \"parameters\":\n",
        "    {\n",
        "        \"maxFailedItems\": 0,\n",
        "        \"maxFailedItemsPerBatch\": 0,\n",
        "        \"configuration\":\n",
        "        {\n",
        "            \"dataToExtract\": \"contentAndMetadata\",\n",
        "            \"imageAction\": \"generateNormalizedImages\"\n",
        "        }\n",
        "    }}\n",
        "\n",
        "    r = requests.put(endpoint + \"/indexers/\" + indexer_name,\n",
        "                data=json.dumps(indexer_payload), headers=headers, params=params)\n",
        "\n",
        "    # Get indexer status\n",
        "    r = requests.get(endpoint + \"/indexers/\" + indexer_name +\n",
        "                \"/status\", headers=headers, params=params)\n",
        "    print(\"287\")\n",
        "    s_indexer = r.json()\n",
        "    time.sleep(60)\n",
        "    while s_indexer[\"lastResult\"][\"itemsProcessed\"] != no_of_docs:\n",
        "        time.sleep(20)\n",
        "        # Get indexer status\n",
        "        r = requests.get(endpoint + \"/indexers/\" + indexer_name +\n",
        "                \"/status\", headers=headers, params=params)\n",
        "        s_indexer = r.json()\n",
        "    print(\"295\")\n",
        "    # Query the index to return the contents of organizations\n",
        "    r = requests.get(endpoint + \"/indexes/\" + index_name +\n",
        "                \"/docs?&search=\"+search_term, headers=headers, params=params)\n",
        "\n",
        "    s = r.json()\n",
        "\n",
        "    search_score = []\n",
        "    doc_name = []\n",
        "    for k1,v1 in s.items():\n",
        "        if k1 == \"value\":\n",
        "            for i in range(len(v1)):\n",
        "                for k2,v2 in v1[i].items():\n",
        "                    if k2 == \"@search.score\":\n",
        "                        print(\"Search score is {}\".format(v2))\n",
        "                        search_score.append(v2)\n",
        "                    elif k2 == \"metadata_storage_name\":\n",
        "                        print(\"Document name is {}\".format(v2))\n",
        "                        doc_name.append(v2)\n",
        "                        print(\"*\" * (15))\n",
        "                    else:\n",
        "                        pass\n",
        "    data2 = {\"FileName\" : doc_name, \"SearchScore\" : search_score}\n",
        "    df2 = pd.DataFrame(data2, columns = ['FileName', 'SearchScore'])\n",
        "\n",
        "    df3 = pd.merge(left=df1, right=df2, left_on='FileName', right_on='FileName')\n",
        "\n",
        "    df3 = df3.sort_values(by = ['SearchScore'], ascending=False, ignore_index=True)\n",
        "\n",
        "    df3[\"Label\"] = str(search)\n",
        "    \n",
        "    #print(\"The result file is downloaded at {}\".format(rootdir + 'Result.csv'))\n",
        "\n",
        "    # delete the skillset\n",
        "    r = requests.delete(endpoint + \"/skillsets/\" + skillset_name,\n",
        "                    headers=headers, params=params)\n",
        "\n",
        "\n",
        "    return df3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipAYBeSPj79x"
      },
      "source": [
        "**TEST_PROTOTYPE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv0gHrWTj4Mh"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    Dep_name = input()\n",
        "    User_id = input()\n",
        "    rootdir = input()\n",
        "    search = input()\n",
        "    labels_m1 = {0 : \"religion\", 1 : \"computers\", 2 : \"sale\", 3 : \"recreation\", 4 : \"science\", 5 : \"politics\"}\n",
        "    rlabels_m1 = {\"religion\" : 0, \"computers\" : 1, \"sale\" : 2, \"recreation\" : 3, \"science\" : 4, \"politics\" : 5}\n",
        "    labels_m2 = {0:\"bills\", 1:\"case study\", 2:\"coding guidelines\", 3:\"product management toolkit\", 4 : \"rules and regulations\", 5 : \"faq\"}\n",
        "    rlabels_m2 = {\"bills\" : 0, \"case study\" : 1, \"coding guidelines\" : 2, \"product management toolkit\" : 3, \"rules and regulations\" : 4, \"faq\" :5}\n",
        "    \n",
        "    if Dep_name == \"Department 1\" and rlabels_m1[search]:\n",
        "        # Model 1\n",
        "        print(\"21\")\n",
        "        Data_Frame = model_1(rootdir, search, labels_m1, rlabels_m1)\n",
        "        Data_Frame.to_csv(User_id + 'result.csv', encoding='utf-8', index = False)\n",
        "    elif Dep_name == \"Department 2\" and rlabels_m2[search]:\n",
        "        # Model 2\n",
        "        Data_Frame = model_2(rootdir, search, labels_m2, rlabels_m2)\n",
        "        Data_Frame.to_csv(User_id + 'result.csv', encoding='utf-8', index = False)\n",
        "    else:\n",
        "        # Active Learning\n",
        "        Data_Frame = act_learn(rootdir, search, Dep_name, User_id)\n",
        "        Data_Frame.to_csv(User_id + 'result.csv', encoding='utf-8', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}