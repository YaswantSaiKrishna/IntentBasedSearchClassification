{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HybridModel_ProtoType.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW0I7m0fiUQL",
        "outputId": "3fed059e-e9b8-4531-fb32-7d1058f3fcbc"
      },
      "source": [
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
        "!pip install fastai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
            "Collecting torch_nightly\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.2.0.dev20190805%2Bcu92-cp36-cp36m-linux_x86_64.whl (704.8MB)\n",
            "\u001b[K     |████████████████████████████████| 704.8MB 20kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-nightly\n",
            "Successfully installed torch-nightly-1.2.0.dev20190805+cu92\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.61)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (20.4)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.8.1+cu101)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.7.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.7.0+cu101)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.7)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (2.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (7.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWS-jkO1iVdV",
        "outputId": "a340c4f7-da94-4674-9e79-aea5c83a94f5"
      },
      "source": [
        "!apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \\\n",
        "flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig libpulse-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "libjpeg-dev set to manually installed.\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "libxml2-dev is already the newest version (2.9.4+dfsg1-6.1ubuntu1.3).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libid3tag0 libijs-0.35 libjbig2dec0\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libpulse-mainloop-glib0 libsox-fmt-alsa libsox-fmt-base libsox3 poppler-data\n",
            "  swig3.0 tesseract-ocr-eng tesseract-ocr-osd\n",
            "Suggested packages:\n",
            "  fonts-noto ghostscript-x lame-doc file libsox-fmt-all fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum swig-doc swig-examples\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  antiword flac fonts-droid-fallback fonts-noto-mono ghostscript gsfonts lame\n",
            "  libcupsfilters1 libcupsimage2 libgs9 libgs9-common libid3tag0 libijs-0.35\n",
            "  libjbig2dec0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
            "  libopencore-amrwb0 libpulse-dev libpulse-mainloop-glib0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox-fmt-mp3 libsox3 libxslt1-dev poppler-data\n",
            "  poppler-utils pstotext sox swig swig3.0 tesseract-ocr tesseract-ocr-eng\n",
            "  tesseract-ocr-osd unrtf\n",
            "0 upgraded, 36 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 21.9 MB of archives.\n",
            "After this operation, 83.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 antiword amd64 0.37-11build1 [128 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 flac amd64 1.3.2-1 [144 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.13 [5,092 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.13 [2,263 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.13 [51.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 lame amd64 3.100-2 [47.8 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.11 [22.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.11 [81.5 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3ubuntu0.18.04.1 [15.9 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxslt1-dev amd64 1.1.29-5ubuntu0.2 [407 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pstotext amd64 1.9-6build1 [32.4 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/universe amd64 unrtf amd64 0.21.9-clean-3 [43.3 kB]\n",
            "Fetched 21.9 MB in 3s (7,037 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../02-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../03-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../04-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../05-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package antiword.\n",
            "Preparing to unpack .../06-antiword_0.37-11build1_amd64.deb ...\n",
            "Unpacking antiword (0.37-11build1) ...\n",
            "Selecting previously unselected package flac.\n",
            "Preparing to unpack .../07-flac_1.3.2-1_amd64.deb ...\n",
            "Unpacking flac (1.3.2-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../08-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../09-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../10-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../11-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../12-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.13_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.13) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../13-libgs9_9.26~dfsg+0-0ubuntu0.18.04.13_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.13) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../14-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.13_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.13) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../15-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package lame.\n",
            "Preparing to unpack .../16-lame_3.100-2_amd64.deb ...\n",
            "Unpacking lame (3.100-2) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../17-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../18-libid3tag0_0.15.1b-13_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../19-libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "Preparing to unpack .../20-libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.11_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../21-libpulse-dev_1%3a11.1-1ubuntu7.11_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../22-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../23-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../24-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../25-libsox-fmt-mp3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libxslt1-dev:amd64.\n",
            "Preparing to unpack .../26-libxslt1-dev_1.1.29-5ubuntu0.2_amd64.deb ...\n",
            "Unpacking libxslt1-dev:amd64 (1.1.29-5ubuntu0.2) ...\n",
            "Selecting previously unselected package poppler-utils.\n",
            "Preparing to unpack .../27-poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Selecting previously unselected package pstotext.\n",
            "Preparing to unpack .../28-pstotext_1.9-6build1_amd64.deb ...\n",
            "Unpacking pstotext (1.9-6build1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../29-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../30-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../31-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../32-tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../33-tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../34-tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Selecting previously unselected package unrtf.\n",
            "Preparing to unpack .../35-unrtf_0.21.9-clean-3_amd64.deb ...\n",
            "Unpacking unrtf (0.21.9-clean-3) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.13) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up unrtf (0.21.9-clean-3) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up flac (1.3.2-1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up lame (3.100-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.13) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up antiword (0.37-11build1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libxslt1-dev:amd64 (1.1.29-5ubuntu0.2) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.13) ...\n",
            "Setting up pstotext (1.9-6build1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l_1wWh4iiVqa",
        "outputId": "cf197326-4021-429a-8b94-7547616e196c"
      },
      "source": [
        "!pip install text-preprocessing\n",
        "!pip install textract\n",
        "!pip install azure-storage-blob"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting text-preprocessing\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/d7/0fd129ae41ce69be93cc047269f9ad088e8712c5e5a7fd1d7147bdb4d49d/text_preprocessing-0.0.9-py2.py3-none-any.whl\n",
            "Collecting unittest-xml-reporting\n",
            "  Downloading https://files.pythonhosted.org/packages/3a/da/cfe167186083aee23122d7bc5978d1064767c794637cbf0debff8762c8ab/unittest_xml_reporting-3.0.4-py2.py3-none-any.whl\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/ad/d1c685967945a04f8596128b15a1ab56c51488f53312e953341af6ff22d1/contractions-0.0.43-py2.py3-none-any.whl\n",
            "Collecting names-dataset\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/98/a4f5ac031a8092b0de61097aad407ce78df2574df6dc03143a76574be726/names_dataset-1.9.1-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from text-preprocessing) (3.2.5)\n",
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/96/827c132397d0eb5731c1eda05dbfb019ede064ca8c7d0f329160ce0a4acd/pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 44.9MB/s \n",
            "\u001b[?25hCollecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->text-preprocessing) (1.15.0)\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 46.7MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81697 sha256=98ba1b8194dfcdcd02a3df1b09a1a948564b0b837c76b459bdd8192c3c500df0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: unittest-xml-reporting, pyahocorasick, Unidecode, textsearch, contractions, names-dataset, pyspellchecker, text-preprocessing\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.43 names-dataset-1.9.1 pyahocorasick-1.4.0 pyspellchecker-0.5.5 text-preprocessing-0.0.9 textsearch-0.0.17 unittest-xml-reporting-3.0.4\n",
            "Collecting textract\n",
            "  Downloading https://files.pythonhosted.org/packages/32/31/ef9451e6e48a1a57e337c5f20d4ef58c1a13d91560d2574c738b1320bb8d/textract-1.6.3-py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.9MB/s \n",
            "\u001b[?25hCollecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting docx2txt==0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Collecting argcomplete==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/82/f44c9661e479207348a979b1f6f063625d11dc4ca6256af053719bbb0124/argcomplete-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from textract) (3.0.4)\n",
            "Collecting pdfminer.six==20181108\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 11.2MB/s \n",
            "\u001b[?25hCollecting python-pptx==0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 26.3MB/s \n",
            "\u001b[?25hCollecting extract-msg==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/90/84485a914ed90adb5e87df17e626be04162fbba146dfecf34643659a4633/extract_msg-0.23.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hCollecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 48.7MB/s \n",
            "\u001b[?25hCollecting SpeechRecognition==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 97kB/s \n",
            "\u001b[?25hCollecting EbookLib==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 49.1MB/s \n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (2.3.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/6f/7e38d7c97fbbc3987539c804282c33f56b6b07381bf2390deead696440c5/pycryptodome-3.9.9-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 252kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (7.0.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/41/bf1aae04932d1eaffee1fc5f8b38ca47bbbf07d765129539bc4bcce1ce0c/XlsxWriter-1.3.7-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 51.8MB/s \n",
            "\u001b[?25hCollecting olefile==0.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Collecting imapclient==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/39/e1c2c2c6e2356ab6ea81fcfc0a74b044b311d6a91a45300811d9a6077ef7/IMAPClient-2.1.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Building wheels for collected packages: docx2txt, python-pptx, EbookLib, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3965 sha256=17b435a8ad4f24b3a9e188b1f2e456d7b922d7df0e70e76917ba31994927b04c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp36-none-any.whl size=275707 sha256=9208745e8ae94b587d54bab57b9c6c660d28c332774fb095d2f254043a98a71e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.17.1-cp36-none-any.whl size=38163 sha256=a6a517c87a7bbdb3d1a6adffbd6e6220e377e92345609f1ec762b51da07dddd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35415 sha256=94be637baf05c1e55f3b90f45e677f055cacbc7411648d977d325c177c6f14dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built docx2txt python-pptx EbookLib olefile\n",
            "\u001b[31mERROR: nbclient 0.5.1 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: soupsieve, beautifulsoup4, six, docx2txt, argcomplete, pycryptodome, pdfminer.six, XlsxWriter, python-pptx, olefile, imapclient, extract-msg, xlrd, SpeechRecognition, EbookLib, textract\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.3.7 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.9.9 python-pptx-0.6.18 six-1.12.0 soupsieve-2.0.1 textract-1.6.3 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-storage-blob\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/5d/0bb4ed37da2523c393789b1d8ecbf56b1d35fa344af30fe423da2c06cbe9/azure_storage_blob-12.6.0-py2.py3-none-any.whl (328kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 7.6MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/f5/9e315fe8cb985b0ce052b34bcb767883dc739f46fadb62f05a7e6d6eedbe/msrest-0.6.19-py2.py3-none-any.whl (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.1MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a2/6565c5271a79e3c96d7a079053b4d8408a740d4bf365f0f5f244a807bd09/cryptography-3.2.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 24.3MB/s \n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/4b/ea7faaafac956a168ab9a95a7ebe583f9d308e8332a68af0ed3128ef520c/azure_core-1.9.0-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob) (1.3.0)\n",
            "Collecting isodate>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob) (2020.11.8)\n",
            "Requirement already satisfied: requests~=2.16 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob) (2.23.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.3)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.12.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-storage-blob) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.10->azure-storage-blob) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.10->azure-storage-blob) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.16->msrest>=0.6.10->azure-storage-blob) (1.24.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob) (2.20)\n",
            "Installing collected packages: isodate, msrest, cryptography, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.9.0 azure-storage-blob-12.6.0 cryptography-3.2.1 isodate-0.6.0 msrest-0.6.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Zr8CYqiqRd",
        "outputId": "23e96dff-ab6c-463b-f561-9110d8e3360b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords', quiet = True)\n",
        "nltk.download('punkt', quiet = True)\n",
        "nltk.download('words',quiet = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBjDMgJwiqyO"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im1JNBIai2tZ"
      },
      "source": [
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.text import * \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from text_preprocessing import preprocess_text\n",
        "import textract\n",
        "from functools import partial\n",
        "import re\n",
        "import io\n",
        "import os"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5j4GuRYjTUx"
      },
      "source": [
        "**MODEL 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpuF7NQQiOSs"
      },
      "source": [
        "def unique_list(l):\n",
        "    ulist = []\n",
        "    [ulist.append(x) for x in l if x not in ulist]\n",
        "    return ulist\n",
        "\n",
        "def process(rootdir, train = True):\n",
        "  #File path.\n",
        "  paths = []\n",
        "  #File name. \n",
        "  fname = []\n",
        "  #Textracted content from file.\n",
        "  descr = []\n",
        "  #Labels of file.\n",
        "  label = []\n",
        "  #Length of each document.\n",
        "  length = []\n",
        "  # Store the list of stop words in english language.\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # Store the list of words common in english language.  \n",
        "  words = set(nltk.corpus.words.words())\n",
        "\n",
        "  # Walking through folders & subfolders in the root directory.\n",
        "  for subdir, dirs, files in os.walk(rootdir):\n",
        "    extract_label = subdir.split('/')\n",
        "    for file in files:\n",
        "        # Append the path of the file to path variable.\n",
        "        paths.append(os.path.join(subdir, file))\n",
        "        # Append the filename to the filename variable.\n",
        "        fname.append(str(file))\n",
        "        # Append the label to label variable.\n",
        "        label.append(0) #[Optional May Change Depending on your folder structure].\n",
        "\n",
        "        # Extract the text from the files and decode the byte string to text.\n",
        "        text = textract.process(os.path.join(subdir, file)).decode(\"utf-8\") \n",
        "        \n",
        "        # Preprocess the text with a custom library. (Includes stemming, lemmatization, removal of special characters).\n",
        "        t = preprocess_text(text)\n",
        "  \n",
        "        # Tokenize the text.\n",
        "        word_tokens = word_tokenize(t)  \n",
        "  \n",
        "        # Apply a filter to the text which removes stop words.\n",
        "        filtered_sentence = [w for w in word_tokens if w not in stop_words]  \n",
        "        \n",
        "        # Remove charcaters which are not properly processed in the text.\n",
        "        filtered_sentence = [w for w in filtered_sentence if len(w) > 3]\n",
        "\n",
        "        # Remove any numeric characters that got included in text\n",
        "        filtered_sentence = [''.join(x for x in i if x.isalpha()) for i in filtered_sentence]\n",
        "\n",
        "        # Join the tokens with space(' ') as delimiter.\n",
        "        filtered_sentence = \" \".join(filtered_sentence)\n",
        "\n",
        "        # Remove extra spaces in the text.\n",
        "        res = re.sub(' +', ' ', filtered_sentence) \n",
        "\n",
        "        # Join the text.\n",
        "        a=' '.join(unique_list(res.split()))\n",
        "\n",
        "        # Remove the words that are not present in english language.\n",
        "        a = \" \".join(w for w in nltk.wordpunct_tokenize(a) \\\n",
        "              if w.lower() in words or not w.isalpha())\n",
        "        \n",
        "        # Append the text to description variable.\n",
        "        descr.append(a)\n",
        "\n",
        "        # Append the length of text to length variable.\n",
        "        length.append(len(a))\n",
        "    \n",
        "  if train:\n",
        "    # Converting the target variable to a numpy array.\n",
        "    label = np.array(label)\n",
        "  \n",
        "  return {\"FileName\" : fname, \"FilePath\" : paths, \"Text\" : descr ,\"Label\" : label, \"Length\" : length}\n",
        "\n",
        "\n",
        "def preprocess(Data_Frame):\n",
        "    #Dropping NaN values.\n",
        "    Data_Frame['Text'].isnull().sum()\n",
        "    Data_Frame.dropna(inplace = True)\n",
        "\n",
        "    # Remove column names 'FileName' & 'FilePath from Dataframe for training. \n",
        "    Data_Frame.drop(['Length', 'Label'], axis = 1, inplace = True)\n",
        "\n",
        "    return Data_Frame\n",
        "\n",
        "def model_1(rootdir, search, labels, rlabels):\n",
        "\n",
        "    learn      = load_learner('/content/drive/MyDrive/cf/model1') # Fixed path for model.\n",
        "\n",
        "    Data        =  process(rootdir)\n",
        "    Data_Frame  =  pd.DataFrame(Data, columns = ['FileName', 'FilePath', 'Text' ,'Label', 'Length'])\n",
        "    \n",
        "    Data_Frame = preprocess(Data_Frame)\n",
        "\n",
        "    target = []\n",
        "    for i in range(len(Data_Frame)) : \n",
        "        target.append(learn.predict(str(Data_Frame.loc[i, \"Text\"])))\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(target)):\n",
        "        target1 = target[i][0]\n",
        "        res = int(\"\".join(re.findall(r'\\d+', str(target1))))\n",
        "        result.append(labels[res])\n",
        "\n",
        "    Data_Frame[\"target\"] = result\n",
        "\n",
        "    if search == \"*\":\n",
        "        Data_Frame.to_csv('result.csv', encoding='utf-8', index = False)\n",
        "    else:\n",
        "        Data_Frame = Data_Frame.loc[Data_Frame['target'] == rlabels[search]]\n",
        "        #reseting index for test_data\n",
        "        Data_Frame.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return Data_Frame"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tG8bySgjXQE"
      },
      "source": [
        "**MODEL 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYde1c1hjL0n"
      },
      "source": [
        "def model_2(rootdir, search, labels, rlabels):\n",
        "\n",
        "    learn = load_learner('/content/drive/MyDrive/cf/model2') # Fixed path for model.\n",
        "\n",
        "    Data        =  process(rootdir)\n",
        "    Data_Frame  =  pd.DataFrame(Data, columns = ['FileName', 'FilePath', 'Text' ,'Label', 'Length'])\n",
        "    \n",
        "    Data_Frame = preprocess(Data_Frame)\n",
        "\n",
        "    target = []\n",
        "    for i in range(len(Data_Frame)) : \n",
        "        target.append(learn.predict(str(Data_Frame.loc[i, \"Text\"])))\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(target)):\n",
        "        target1 = target[i][0]\n",
        "        res = int(\"\".join(re.findall(r'\\d+', str(target1))))\n",
        "        result.append(labels[res])\n",
        "\n",
        "    Data_Frame[\"target\"] = result\n",
        "\n",
        "    if search == \"*\":\n",
        "        Data_Frame.to_csv('result.csv', encoding='utf-8', index = False)\n",
        "    else:\n",
        "        Data_Frame = Data_Frame.loc[Data_Frame['target'] == rlabels[search]]\n",
        "        #reseting index for test_data\n",
        "        Data_Frame.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return Data_Frame"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbFsBp9vji56"
      },
      "source": [
        "**ACTIVE LEARNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8WRCce-jgll"
      },
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import urllib.parse"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPNMLHpujwZ1"
      },
      "source": [
        "def act_learn(rootdir, search, cont_name, folder):\n",
        "    container_names = {\"Department 1\" : \"department1\", \"Department 2\" : \"department2\"}\n",
        "    cont_name = container_names[cont_name]\n",
        "    paths = []\n",
        "    fname = []\n",
        "    search_term = urllib.parse.quote(str(search)) \n",
        "    print(\"File is being downloaded it may take upto 5 minutes\")\n",
        "    for subdir, dirs, files in os.walk(rootdir):\n",
        "        for file in files:\n",
        "            paths.append(os.path.join(subdir, file))\n",
        "            fname.append(str(file))\n",
        "    no_of_docs = int(len(fname))\n",
        "    \n",
        "    data1 = {\"FileName\" : fname, \"FilePath\" : paths, \"Label\" : [None] * len(paths)}\n",
        "    df1 = pd.DataFrame(data1, columns = ['FileName', 'FilePath', 'Label'])\n",
        "    \n",
        "    # Retrieve the connection string for use with the application. \n",
        "    connect_str = \"<Your Azure Storage Connection String>\"\n",
        "\n",
        "    # Create the BlobServiceClient object which will be used to create a container client\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "    for ind in df1.index:\n",
        "        #Uploading file to azure Blob Storage\n",
        "        upload_file_path = df1['FilePath'][ind]\n",
        "        local_file_name = str(folder) + \"/\" + df1['FileName'][ind]\n",
        "        # Specify the container (Dynamic Drop Down)\n",
        "        container_name = cont_name\n",
        "\n",
        "        # Create a blob client using the local file name as the name for the blob\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=local_file_name)\n",
        "\n",
        "        # Upload the created file\n",
        "        with open(upload_file_path, \"rb\") as data:\n",
        "            blob_client.upload_blob(data)\n",
        "\n",
        "    # Define the names for the data source, skillset, index and indexer\n",
        "    datasource_name = \"cogsrch-py-datasource\" + str(folder)\n",
        "    skillset_name = \"cogsrch-py-skillset\" + str(folder)\n",
        "    index_name = \"cogsrch-py-index\" + str(folder)\n",
        "    indexer_name = \"cogsrch-py-indexer\" + str(folder)\n",
        "\n",
        "    # Setup the endpoint\n",
        "    endpoint = 'https://eandysearch.search.windows.net'\n",
        "    headers = {'Content-Type': 'application/json',\n",
        "    'api-key': 'A15802993B784F745D50071F67BC731E'}\n",
        "    params = {'api-version': '2020-06-30'}\n",
        "\n",
        "    # Create a data source\n",
        "    datasourceConnectionString = connect_str\n",
        "    datasource_payload = {\n",
        "    \"name\": datasource_name,\n",
        "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
        "    \"type\": \"azureblob\",\n",
        "    \"credentials\": {\n",
        "        \"connectionString\": datasourceConnectionString\n",
        "    },\n",
        "    \"container\": {\n",
        "        \"name\": cont_name,\n",
        "        \"query\" : str(folder) + \"/\"\n",
        "    }}\n",
        "    r = requests.put(endpoint + \"/datasources/\" + datasource_name,\n",
        "                data=json.dumps(datasource_payload), headers=headers, params=params)\n",
        "    \n",
        "    # Create a skillset\n",
        "    skillset_payload = {\n",
        "    \"name\": skillset_name,\n",
        "    \"description\":\n",
        "    \"Extract entities, detect language and extract key-phrases\",\n",
        "    \"skills\":\n",
        "    [\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.EntityRecognitionSkill\",\n",
        "            \"categories\": [\"Organization\"],\n",
        "            \"defaultLanguageCode\": \"en\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\", \n",
        "                    \"source\": \"/document/content\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"organizations\", \n",
        "                    \"targetName\": \"organizations\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\", \n",
        "                    \"source\": \"/document/content\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"languageCode\",\n",
        "                    \"targetName\": \"languageCode\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
        "            \"textSplitMode\": \"pages\",\n",
        "            \"maximumPageLength\": 4000,\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\",\n",
        "                    \"source\": \"/document/content\"\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"languageCode\",\n",
        "                    \"source\": \"/document/languageCode\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"textItems\",\n",
        "                    \"targetName\": \"pages\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\",\n",
        "            \"context\": \"/document/pages/*\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\", \n",
        "                    \"source\": \"/document/pages/*\"\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"languageCode\", \n",
        "                    \"source\": \"/document/languageCode\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"keyPhrases\",\n",
        "                    \"targetName\": \"keyPhrases\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]}\n",
        "\n",
        "    r = requests.put(endpoint + \"/skillsets/\" + skillset_name,\n",
        "                data=json.dumps(skillset_payload), headers=headers, params=params)\n",
        "    \n",
        "    # Create an index\n",
        "    index_payload = {\n",
        "    \"name\": index_name,\n",
        "    \"fields\": [\n",
        "        {\n",
        "            \"name\": \"id\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"key\": \"true\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\",\n",
        "            \"sortable\": \"true\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"metadata_storage_name\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"facetable\": \"false\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"key\": \"false\",\n",
        "            \"retrievable\": \"true\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"analyzer\": \"standard.lucene\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"content\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"languageCode\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"keyPhrases\",\n",
        "            \"type\": \"Collection(Edm.String)\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"organizations\",\n",
        "            \"type\": \"Collection(Edm.String)\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        }\n",
        "    ]}\n",
        "\n",
        "    r = requests.put(endpoint + \"/indexes/\" + index_name,\n",
        "                data=json.dumps(index_payload), headers=headers, params=params)\n",
        "\n",
        "    # Create an indexer\n",
        "    indexer_payload = {\n",
        "    \"name\": indexer_name,\n",
        "    \"dataSourceName\": datasource_name,\n",
        "    \"targetIndexName\": index_name,\n",
        "    \"skillsetName\": skillset_name,\n",
        "    \"fieldMappings\": [\n",
        "        {\n",
        "            \"sourceFieldName\": \"metadata_storage_path\",\n",
        "            \"targetFieldName\": \"id\",\n",
        "            \"mappingFunction\":\n",
        "            {\"name\": \"base64Encode\"}\n",
        "        },\n",
        "        {\n",
        "            \"sourceFieldName\": \"content\",\n",
        "            \"targetFieldName\": \"content\"\n",
        "        }\n",
        "    ],\n",
        "    \"outputFieldMappings\":\n",
        "    [\n",
        "        {\n",
        "            \"sourceFieldName\": \"/document/organizations\",\n",
        "            \"targetFieldName\": \"organizations\"\n",
        "        },\n",
        "        {\n",
        "            \"sourceFieldName\": \"/document/pages/*/keyPhrases/*\",\n",
        "            \"targetFieldName\": \"keyPhrases\"\n",
        "        },\n",
        "        {\n",
        "            \"sourceFieldName\": \"/document/languageCode\",\n",
        "            \"targetFieldName\": \"languageCode\"\n",
        "        }\n",
        "    ],\n",
        "    \"parameters\":\n",
        "    {\n",
        "        \"maxFailedItems\": 0,\n",
        "        \"maxFailedItemsPerBatch\": 0,\n",
        "        \"configuration\":\n",
        "        {\n",
        "            \"dataToExtract\": \"contentAndMetadata\",\n",
        "            \"imageAction\": \"generateNormalizedImages\"\n",
        "        }\n",
        "    }}\n",
        "\n",
        "    r = requests.put(endpoint + \"/indexers/\" + indexer_name,\n",
        "                data=json.dumps(indexer_payload), headers=headers, params=params)\n",
        "\n",
        "    # Get indexer status\n",
        "    r = requests.get(endpoint + \"/indexers/\" + indexer_name +\n",
        "                \"/status\", headers=headers, params=params)\n",
        "    \n",
        "    s_indexer = r.json()\n",
        "    time.sleep(200)\n",
        "    '''while s_indexer[\"lastResult\"] != None:\n",
        "      print(\"okay\")\n",
        "      print(s_indexer)\n",
        "      while s_indexer[\"lastResult\"][\"itemsProcessed\"] != no_of_docs:\n",
        "        print([\"lastResult\"][\"itemsProcessed\"])\n",
        "        time.sleep(20)\n",
        "        # Get indexer status\n",
        "        r = requests.get(endpoint + \"/indexers/\" + indexer_name +\n",
        "                \"/status\", headers=headers, params=params)\n",
        "        s_indexer = r.json()'''\n",
        "    \n",
        "    # Query the index to return the contents of organizations\n",
        "    r = requests.get(endpoint + \"/indexes/\" + index_name +\n",
        "                \"/docs?&search=\"+search_term, headers=headers, params=params)\n",
        "\n",
        "    s = r.json()\n",
        "\n",
        "    search_score = []\n",
        "    doc_name = []\n",
        "    for k1,v1 in s.items():\n",
        "        if k1 == \"value\":\n",
        "            for i in range(len(v1)):\n",
        "                for k2,v2 in v1[i].items():\n",
        "                    if k2 == \"@search.score\":\n",
        "                        t = (v2/no_of_docs)*100\n",
        "                        #print(\"Search score is {}\".format(t))\n",
        "                        search_score.append(t)\n",
        "                    elif k2 == \"metadata_storage_name\":\n",
        "                        #print(\"Document name is {}\".format(v2))\n",
        "                        doc_name.append(v2)\n",
        "                        #print(\"*\" * (15))\n",
        "                    else:\n",
        "                        pass\n",
        "    data2 = {\"FileName\" : doc_name, \"SearchScore\" : search_score}\n",
        "    df2 = pd.DataFrame(data2, columns = ['FileName', 'SearchScore'])\n",
        "\n",
        "    df3 = pd.merge(left=df1, right=df2, left_on='FileName', right_on='FileName')\n",
        "\n",
        "    df3 = df3.sort_values(by = ['SearchScore'], ascending=False, ignore_index=True)\n",
        "\n",
        "    df3[\"Label\"] = str(search)\n",
        "    \n",
        "    #print(\"The result file is downloaded at {}\".format(rootdir + 'Result.csv'))\n",
        "\n",
        "    # delete the skillset\n",
        "    #r = requests.delete(endpoint + \"/skillsets/\" + skillset_name,\n",
        "     #               headers=headers, params=params)\n",
        "\n",
        "\n",
        "    return df3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipAYBeSPj79x"
      },
      "source": [
        "**TEST_PROTOTYPE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fyRAXjVQ9wk",
        "outputId": "8e32cc74-e066-49db-e4e4-9eee678fd7a8"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Enter your Workstream/Department name\")\n",
        "    Dep_name = input()\n",
        "    print(\"Enter the User ID\")\n",
        "    User_id = input()\n",
        "    print(\"Enter the path for root directory\")\n",
        "    rootdir = input()\n",
        "    print(\"Enter the search term\")\n",
        "    search = input()\n",
        "    print(\"******Your search is in progress*********\")\n",
        "    labels_m1 = {0 : \"religion\", 1 : \"computers\", 2 : \"sale\", 3 : \"recreation\", 4 : \"science\", 5 : \"politics\", 6 : \"*\"}\n",
        "    rlabels_m1 = {\"religion\" : 0, \"computers\" : 1, \"sale\" : 2, \"recreation\" : 3, \"science\" : 4, \"politics\" : 5, \"*\" : 6}\n",
        "    labels_m2 = {0:\"bills\", 1:\"case study\", 2:\"coding guidelines\", 3:\"product management toolkit\", 4 : \"rules and regulations\", 5 : \"faq\", 6 : \"*\"}\n",
        "    rlabels_m2 = {\"bills\" : 0, \"case study\" : 1, \"coding guidelines\" : 2, \"product management toolkit\" : 3, \"rules and regulations\" : 4, \"faq\" :5, \"*\" : 6}\n",
        "    \n",
        "    if (Dep_name == \"Department 1\") and (search in rlabels_m1):\n",
        "        # Model 1\n",
        "        Data_Frame = model_1(rootdir, search, labels_m1, rlabels_m1)\n",
        "        Data_Frame.to_csv(User_id + 'result.csv', encoding='utf-8', index = False)\n",
        "    elif (Dep_name == \"Department 2\") and (search in rlabels_m2):\n",
        "        # Model 2\n",
        "        Data_Frame = model_2(rootdir, search, labels_m2, rlabels_m2)\n",
        "        Data_Frame.to_csv(User_id + 'result.csv', encoding='utf-8', index = False)\n",
        "    else:\n",
        "        # Active Learning\n",
        "        Data_Frame = act_learn(rootdir, search, Dep_name, User_id)\n",
        "        Data_Frame.to_csv(User_id + 'result.csv', encoding='utf-8', index = False)\n",
        "    result_path = rootdir+'/'+User_id+'result.csv'\n",
        "    print(\"Your search is completed and the CSV file is downloaded at : {}\".format(result_path))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your Workstream/Department name\n",
            "Department 1\n",
            "Enter the User ID\n",
            "17546\n",
            "Enter the path for root directory\n",
            "/content/sample_data/active_learning\n",
            "Enter the search term\n",
            "azure\n",
            "******Your search is in progress*********\n",
            "File is being downloaded it may take upto 5 minutes\n",
            "Your search is completed and the CSV file is downloaded at : /content/sample_data/active_learning/17546result.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "nUR9YulAQR3y",
        "outputId": "4648dd41-1fa6-49fd-b0ec-61b7812aa92a"
      },
      "source": [
        "# makes the passed rows header \n",
        "result_df = pd.read_csv('/content/'+User_id+'result.csv')\n",
        "result_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FileName</th>\n",
              "      <th>FilePath</th>\n",
              "      <th>Label</th>\n",
              "      <th>SearchScore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C# coding standards.rtf</td>\n",
              "      <td>/content/sample_data/active_test/C# coding sta...</td>\n",
              "      <td>coding standards</td>\n",
              "      <td>99.310433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ProCom.doc</td>\n",
              "      <td>/content/sample_data/active_test/ProCom.doc</td>\n",
              "      <td>coding standards</td>\n",
              "      <td>65.950108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Doc.pdf</td>\n",
              "      <td>/content/sample_data/active_test/Doc.pdf</td>\n",
              "      <td>coding standards</td>\n",
              "      <td>26.907631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>azure-250215.pptx</td>\n",
              "      <td>/content/sample_data/active_test/azure-250215....</td>\n",
              "      <td>coding standards</td>\n",
              "      <td>17.392536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  FileName  ... SearchScore\n",
              "0  C# coding standards.rtf  ...   99.310433\n",
              "1               ProCom.doc  ...   65.950108\n",
              "2                  Doc.pdf  ...   26.907631\n",
              "3        azure-250215.pptx  ...   17.392536\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhgs6rhqSs7x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}